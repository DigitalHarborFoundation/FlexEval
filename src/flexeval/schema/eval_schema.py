# originally generated by datamodel-codegen:
#   filename:  src/flexeval/eval_schema.json
#   timestamp: 2025-05-19T21:42:39+00:00

from __future__ import annotations

import sys

from typing import Any, Dict, List, Literal, Optional

from pydantic import BaseModel, Field

from flexeval.schema import schema_utils


class DependsOnItem(BaseModel):
    class Config:
        extra = "forbid"

    name: Optional[str] = Field(
        None, description="Name of the dependency function or rubric."
    )
    type: Optional[Literal["function", "rubric"]] = Field(
        None,
        description="One of 'function' or 'rubric' indicating the type of the dependency.",
    )
    kwargs: Optional[Dict[str, Any]] = Field(
        None,
        description="The keyword arguments for the dependency. If provided, used to match which evaluation this dependency is for, so must match the keyword args given for some evaluation.",
    )
    context_only: Optional[bool] = Field(
        None,
        description="The context_only value for the dependency. If provided, used to match which evaluation this dependency is for.",
    )
    last_turn_only: Optional[bool] = Field(
        None,
        description="The last_turn_only value for the dependency. If provided, used to match which evaluation this dependency is for.",
    )
    metric_name: Optional[str] = Field(
        None,
        description="Name of the metric dependency. This may be different than function_name if the metric function returns a key/value pair - in which case, this will match the key.",
    )
    metric_min_value: Optional[float] = Field(
        -sys.float_info.max,
        description="Minimum value of the dependency to consider it as satisfied.",
    )
    metric_max_value: Optional[float] = Field(
        sys.float_info.max,
        description="Maximum value of the dependency to consider it as satisfied.",
    )


class MetricItem(BaseModel):
    name: str = Field(
        ...,
        description="The function to call or name of rubric to use to compute this metric.",
    )
    depends_on: Optional[List[DependsOnItem]] = Field(
        default_factory=list,
        description="List of dependencies that must be satisfied for this metric to be computed.",
    )
    metric_level: Optional[str] = Field(
        "Turn",
        description="What level of granularity (ToolCall, Message, Turn, or Thread) this rubric should be applied to",
    )
    context_only: bool = Field(
        False,
        description="If true, only the context (that is, the previous messages) will be evaluated, not the current object. Cannot be done with only thread",
    )
    last_instance_only: bool = Field(
        False,
        description="If true, the object will only be evaluated if it's the last instance (i.e., turn or message depending on metric_level) in an existing conversation, or if it's a new completion.",
    )


class FunctionItem(MetricItem):
    kwargs: schema_utils.OptionalDict = Field(
        default_factory=dict,
        description="Keyword arguments for the function. Each key must correspond to an argument in the function. Extra keys will cause an error.",
    )
    # TODO add the ability to provide a function source: Path | FunctionsCollection | schema_utils.ModuleType


class RubricItem(MetricItem):
    # TODO is RubricItem.kwargs actually used?
    kwargs: Optional[Dict[str, Any]] = Field(
        default_factory=dict,
        description="Keyword arguments for the rubric evaluation.",
    )
    # TODO add the ability to provide a rubric source: Path | RubricsCollection


class Metrics(BaseModel):
    function: Optional[List[FunctionItem]] = Field(
        None, description="List of function-based metrics to be evaluated."
    )
    rubric: Optional[List[RubricItem]] = Field(
        None, description="List of rubrics to be evaluated"
    )


class CompletionLlm(BaseModel):
    class Config:
        extra = "forbid"

    function_name: str = Field(
        ...,
        description="Completion function defined in `completion_functions.py`. Must be specified.",
    )
    include_system_prompt: Optional[bool] = False
    kwargs: Optional[Dict[str, Any]] = Field(
        default_factory=dict,
        description="Additional arguments that will be passed to the completion function. Must correspond to arguments in tne named function.",
    )


class GraderLlm(BaseModel):
    class Config:
        extra = "forbid"

    function_name: str = Field(
        ...,
        description="Function defined in `completion_functions.py`. We're not really completing a conversation, but we ARE asking an LLM to provide a response to an input - in this case, the rubric.",
    )
    kwargs: Optional[Dict[str, Any]] = Field(
        default_factory=dict,
        description="Additional arguments that will be passed to the completion function. Must correspond to arguments in tne named function.",
    )


class Eval(BaseModel):
    class Config:
        # TODO don't permit additional fields in Eval
        extra = "allow"

    do_completion: bool = Field(
        False,
        description="Flag to determine if completions should be done for each conversation. Set to 'true' if you are testing a new API and want to evaluate the API responses. Set to 'false' (default) if you are evaluating past conversations and do not need to generate new completions.",
    )
    name: Optional[str] = Field(
        None,
        description="Name of the test suite. Used as metadata only. Does not need to match the key of the entry in the evals.yaml file.",
    )
    notes: Optional[str] = Field(
        "",
        description="Additional notes regarding the configuration. Used as metadata only.",
    )
    metrics: Metrics = Field(
        default_factory=Metrics, description="Metrics to use in the evaluation."
    )
    completion_llm: Optional[CompletionLlm] = Field(
        None,
        description="Specification of the LLM or API used to perform new completions. Must be defined if `do_completions: true` is set.",
    )
    grader_llm: Optional[GraderLlm] = Field(
        None,
        description="Specification of the LLM or API used to grade rubrics. Must be defined if any rubric_metrics are specified.",
    )
