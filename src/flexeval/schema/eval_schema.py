# generated by datamodel-codegen:
#   filename:  eval_schema.json
#   timestamp: 2025-05-19T21:42:39+00:00

from __future__ import annotations

from typing import Any, Dict, List, Optional, Literal

from pydantic import BaseModel, Field


class Config(BaseModel):
    class Config:
        extra = "allow"

    max_workers: Optional[int] = Field(
        None,
        description="The maximum number of worker threads allowed when computing metrics.",
    )


class DependsOnItem(BaseModel):
    class Config:
        extra = "forbid"

    name: Optional[str] = Field(
        None, description="Name of the dependency function or rubric."
    )
    type: Optional[Literal["function", "rubric"]] = Field(
        None,
        description="One of 'function' or 'rubric' indicating the type of the dependency.",
    )
    kwargs: Optional[Dict[str, Any]] = Field(
        None,
        description="The keyword arguments for the dependency. If provided, used to match which evaluation this dependency is for, so must match the keyword args given for some evaluation.",
    )
    context_only: Optional[bool] = Field(
        None,
        description="The context_only value for the dependency. If provided, used to match which evaluation this dependency is for.",
    )
    last_turn_only: Optional[bool] = Field(
        None,
        description="The last_turn_only value for the dependency. If provided, used to match which evaluation this dependency is for.",
    )
    metric_name: Optional[str] = Field(
        None,
        description="Name of the metric dependency. This may be different than function_name if the metric function returns a key/value pair - in which case, this will match the key.",
    )
    metric_min_value: Optional[float] = Field(
        "-1e20",
        description="Minimum value of the dependency to consider it as satisfied.",
    )
    metric_max_value: Optional[float] = Field(
        "1e20",
        description="Maximum value of the dependency to consider it as satisfied.",
    )


class FunctionItem(BaseModel):
    name: str = Field(..., description="The function to call to compute this metric.")
    kwargs: Optional[Dict[str, Any]] = Field(
        {},
        description="Keyword arguments for the function. Each key must correspond to an argument in the function as implemented in `function_metrics.py`. Extra keys will cause an error.",
    )
    depends_on: Optional[List[DependsOnItem]] = Field(
        [],
        description="List of dependencies that must be satisfied for this metric to be computed.",
    )
    metric_level: Optional[str] = Field(
        "Turn",
        description="What level of granularity (ToolCall, Message, Turn, or Thread) this rubric should be applied to",
    )
    context_only: Optional[bool] = Field(
        False,
        description="If true, only the context (that is, the previous messages) will be evaluated, not the current object. Cannot be done with only thread",
    )
    last_instance_only: Optional[bool] = Field(
        False,
        description="If true, the object will only be evaluated if it's the last instance (i.e., turn or message depending on metric_level) in an existing conversation, or if it's a new completion.",
    )


class RubricItem(BaseModel):
    name: str = Field(..., description="The rubric to use to evaluate this metric.")
    kwargs: Optional[Dict[str, Any]] = Field(
        {},
        description="Keyword arguments for the function. Each key must correspond to an argument in the function as implemented in `function_metrics.py`. Extra keys will cause an error.",
    )
    metric_level: Optional[str] = Field(
        "Turn",
        description="What level of granularity (ToolCall, Message, Turn, or Thread) this rubric should be applied to",
    )
    context_only: Optional[bool] = Field(
        False,
        description="If true, only the context (that is, the previous messages) will be evaluated, not the current object. Cannot be done with only thread",
    )
    last_instance_only: Optional[bool] = Field(
        False,
        description="If true, the object will only be evaluated if it's the last instance (i.e., turn or message depending on metric_level) in an existing conversation, or if it's a new completion.",
    )
    depends_on: Optional[List[DependsOnItem]] = Field(
        [],
        description="List of dependencies that must be satisfied for this metric to be computed.",
    )


class Metrics(BaseModel):
    function: Optional[List[FunctionItem]] = Field(
        None, description="List of function-based metrics to be evaluated."
    )
    rubric: Optional[List[RubricItem]] = Field(
        None, description="List of rubrics to be evaluated"
    )


class CompletionLlm(BaseModel):
    class Config:
        extra = "forbid"

    function_name: str = Field(
        ...,
        description="Completion function defined in `completion_functions.py`. Must be specified.",
    )
    include_system_prompt: Optional[bool] = False
    kwargs: Optional[Dict[str, Any]] = Field(
        {},
        description="Additional arguments that will be passed to the completion function. Must correspond to arguments in tne named function.",
    )


class GraderLlm(BaseModel):
    class Config:
        extra = "forbid"

    function_name: str = Field(
        ...,
        description="Function defined in `completion_functions.py`. We're not really completing a conversation, but we ARE asking an LLM to provide a response to an input - in this case, the rubric.",
    )
    kwargs: Optional[Dict[str, Any]] = Field(
        {},
        description="Additional arguments that will be passed to the completion function. Must correspond to arguments in tne named function.",
    )


class Eval(BaseModel):
    class Config:
        extra = "allow"

    data: List[str] = Field(
        ...,
        description="List of absolute or relative paths to data files. Each file must be in *.jsonl format, with one conversation per line.",
    )
    do_completion: Optional[bool] = Field(
        False,
        description="Flag to determine if completions should be done for each conversation. Set to 'true' if you are testing a new API and want to evaluate the API responses. Set to 'false' (default) if you are evaluating past conversations and do not need to generate new completions.",
    )
    name: Optional[str] = Field(
        "",
        description="Name of the test suite. Used as metadata only. Does not need to match the key of the entry in the evals.yaml file.",
    )
    notes: Optional[str] = Field(
        "",
        description="Additional notes regarding the configuration. Used as metadata only.",
    )
    config: Optional[Config] = Field(
        None,
        description="Specific configuration settings that may override default settings. Look in `src/flexeval/config.yaml` for other fun things to put here.",
    )
    metrics: Metrics
    completion_llm: Optional[CompletionLlm] = Field(
        None,
        description="Specification of the LLM or API used to perform new completions. Must be defined if `do_completions: true` is set.",
    )
    grader_llm: Optional[GraderLlm] = Field(
        None,
        description="Specification of the LLM or API used to grade rubrics. Must be defined if any rubric_metrics are specified.",
    )
