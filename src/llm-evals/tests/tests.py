# TODO - make tests here

# Make a sample dataset then run different types of evals on it
# and make sure they have the correct value and/or form
# in the database output

# string_length by role
# turns by conversation
# strength_length by completion -- where completion function is a dummy that always returns "hi"
# make sure completions work

# test -- evals still works when functions have arguments
# test -- evals still works when functions do NOT have arguments
# test -- evals works when using OpenAI completion function
# test -- evals works when using rubric metric
# test -- evals BREAK when rubric template is incorrectly written  <-- expected failure
