{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512a79f9-25ac-469e-9322-0b199e08436e",
   "metadata": {},
   "source": [
    "(metric_analysis)=\n",
    "# Metric Analysis\n",
    "\n",
    "This vignette demonstrates a more complete use of FlexEval: constructing an {class}`~flexeval.schema.eval_schema.Eval` with both rubric and function metrics, running that eval via an {class}`~flexeval.schema.evalrun_schema.EvalRun`, and using FlexEval utility functions to retrieve and interpret the results.\n",
    "\n",
    "Author: [Zachary Levonian](https://levon003.github.io) \\\n",
    "Date: July 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177fe23-afd0-4548-8fb9-6073308a21cd",
   "metadata": {},
   "source": [
    "## Part 1: Running FlexEval to compute metrics\n",
    "\n",
    "We'll create some test data, build an eval, and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efa0406-0987-42f7-b54a-6833244babbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "assert dotenv.load_dotenv(\"../.env\"), (\n",
    "    \"This vignette assumes access to API keys in a .env file.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2425711-4fee-4d49-bb96-7592a24da32a",
   "metadata": {},
   "source": [
    "### Generating test data\n",
    "\n",
    "Let's evaluate the quality of grade-appropriate explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049336f0-7ede-489b-a45b-5dbb4374912a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts = [\"integer addition\", \"factoring polynomials\", \"logistic regression\"]\n",
    "grades = [\"3rd\", \"5th\", \"7th\", \"9th\"]\n",
    "\n",
    "user_queries = []\n",
    "for concept in concepts:\n",
    "    for grade in grades:\n",
    "        user_queries.append(\n",
    "            f\"Concisely summarize {concept} at the United States {grade}-grade level.\"\n",
    "        )\n",
    "len(user_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71700ecf-1a86-4363-80f8-4919c3c5bbd1",
   "metadata": {},
   "source": [
    "We can imagine that our system under test involves a particular system prompt, or perhaps multiple candidate prompts.\n",
    "\n",
    "In this case, we'll imagine a single, simple system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fdf148-885f-4fb4-bba4-f6f75e915d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a friendly math tutor.\n",
    "\n",
    "You attempt to summarize any mathematical topic the student is interested in, even if it's not appropriate for their grade level.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22992eee-f95e-4254-97b4-a5e7a326e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to JSONL\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "concept_queries_path = Path(\"concept_queries.jsonl\")\n",
    "with open(concept_queries_path, \"w\") as outfile:\n",
    "    for user_query in user_queries:\n",
    "        outfile.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"input\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_query},\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68163ea3-f92c-485a-b5da-6f966c4e4e40",
   "metadata": {},
   "source": [
    "Each line of `concept_queries.jsonl` will become a unique {class}`~flexeval.classes.thread.Thread` to be processed.\n",
    "\n",
    "Now that we have test data, we can build a FlexEval configuration and execute it.\n",
    "\n",
    "### Defining an Eval\n",
    "\n",
    "An {class}`~flexeval.schema.eval_schema.Eval` describes the computations that need to happen to compute the required metrics.\n",
    "\n",
    "In this case, we'll set a few details:\n",
    " - We want to generate new LLM completions, rather than just using any existing assistant messages in our threads. To do that, we'll set {attr}`~flexeval.schema.eval_schema.Eval.do_completion` to true, and define the function to actually generate those completions from those provided in {mod}`flexeval.configuration.completion_functions`. In this case, we'll use {func}`~flexeval.configuration.completion_functions.litellm_completion`, which uses [LiteLLM](https://docs.litellm.ai) to provide access to many different model APIs.\n",
    " - We'll compute two {class}`~flexeval.schema.eval_schema.FunctionItem`s, a [Flesch reading ease](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests) score and {meth}`~flexeval.configuration.function_metrics.is_role`. We need `is_role` because we can use its value to compute particular metrics only for assistant messages (like the new completions we'll be generating).\n",
    " - Finally, we can specify a custom {class}`~flexeval.schema.eval_schema.RubricItem`s. We'll write a prompt that describes the assessment we want to make. In this case, we try to determine if the assistant response is grade appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b72fa23-f077-434a-9916-e6389115560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flexeval\n",
    "from flexeval.schema import (\n",
    "    Eval,\n",
    "    Rubric,\n",
    "    GraderLlm,\n",
    "    DependsOnItem,\n",
    "    Metrics,\n",
    "    FunctionItem,\n",
    "    RubricItem,\n",
    "    CompletionLlm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a3afed-73e7-484d-9a6d-da6758e1716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by specifying an OpenAI model name here, we'll need OPENAI_API_KEY to exist in our environment variables or in our .env file\n",
    "completion_llm = CompletionLlm(\n",
    "    function_name=\"litellm_completion\",\n",
    "    kwargs={\"model\": \"gpt-4o-mini\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b010f4-ab11-4eb8-aae1-ce0f31d37c26",
   "metadata": {},
   "source": [
    "A note about {class}`~flexeval.schema.eval_schema.CompletionLlm`: if you're using LiteLLM, you can replace an API call with a pre-written response by passing `mock_response` as an additional keyword argument.\n",
    "\n",
    "For example, this configuration will always return \"I can't help with that!\":\n",
    "\n",
    "```python\n",
    "kwargs={\"model\": \"gpt-4o-mini\", \"mock_response\": \"I can't help with that!\"}\n",
    "```\n",
    "\n",
    "Now let's define a rubric. We just need a prompt, a set of choice scores mapping from string responses to a numeric metric value, and information about what LLM to use. For simplicity, we'll use the same model for evaluating the completions that we use to generate them â€“ but this should probably be avoided in general due to LLMs' [preference for their own outputs](https://arxiv.org/abs/2404.13076)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccc2343-0ed4-46fa-96ed-f13d09afd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rubric_prompt = \"\"\"Read the following input and output, assessing if the output is grade-appropriate.\n",
    "[Input]: {context}\n",
    "[Output]: {content}\n",
    "\n",
    "On a new line after your explanation, print:\n",
    "- YES if the Output is fully appropriate for the grade level\n",
    "- NO if the Output would uses language or concepts that would be inappropriate for that grade level\n",
    "\n",
    "Only print YES or NO on the final line.\n",
    "\"\"\"\n",
    "rubric = Rubric(\n",
    "    prompt=bad_rubric_prompt,\n",
    "    choice_scores={\"YES\": 1, \"NO\": 0},\n",
    ")\n",
    "rubrics = {\n",
    "    \"is_grade_appropriate\": rubric,\n",
    "}\n",
    "grader_llm = GraderLlm(\n",
    "    function_name=\"litellm_completion\", kwargs={\"model\": \"gpt-4o-mini\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e2fe7-128e-4bd4-8a0c-fd1fc0156813",
   "metadata": {},
   "source": [
    "We'll define the metrics that need to be computed using the rubric name defined above (`is_grade_appropriate`).\n",
    "\n",
    "We'll also define the computation of our two function metrics: {meth}`~flexeval.configuration.function_metrics.is_role` and {meth}`~flexeval.configuration.function_metrics.flesch_reading_ease`.\n",
    "\n",
    "We want to evaluate `flesch_reading_ease` and `is_grade_appropriate` only on assistant turns, so we need to declare a dependency.\n",
    "\n",
    "Here's how we declare a dependency:\n",
    "\n",
    " - A {class}`~flexeval.schema.eval_schema.DependsOnItem` requires a `name` that matches a defined metric.\n",
    " - Providng `kwargs` are optional, but if you use the same name with different kwargs you need to provide them so that we know which metric you're depending on.\n",
    " - Set `metric_min_value` or `metric_max_value` or both. The {meth}`~flexeval.configuration.function_metrics.is_role` documentation tell us that it returns 1 (true) if the turn has the role provided in the keyword args and 0 (false) otherwise. So we just need to set `metric_min_value` to 1.\n",
    " - When we define a {class}`~flexeval.schema.eval_schema.MetricItem` that has dependencies, provide one or more {class}`~flexeval.schema.eval_schema.DependsOnItem`s in the {attr}`~flexeval.schema.eval_schema.MetricItem.depends_on` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11632831-8cd1-4861-8721-4399d70694db",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_assistant_dependency = DependsOnItem(\n",
    "    name=\"is_role\", kwargs={\"role\": \"assistant\"}, metric_min_value=1\n",
    ")\n",
    "metrics = Metrics(\n",
    "    function=[\n",
    "        FunctionItem(name=\"is_role\", kwargs={\"role\": \"assistant\"}),\n",
    "        FunctionItem(\n",
    "            name=\"flesch_reading_ease\",\n",
    "            depends_on=[is_assistant_dependency],\n",
    "        ),\n",
    "    ],\n",
    "    rubric=[\n",
    "        RubricItem(name=\"is_grade_appropriate\", depends_on=[is_assistant_dependency])\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e0ab8-7aa2-4866-a19c-2519fd5a4067",
   "metadata": {},
   "source": [
    "We'll finish building our {class}`~flexeval.schema.eval_schema.Eval` by providing all the info we defined above.\n",
    "\n",
    "An eval's `name` is optional, but providing one can help you later if you're running lots of different Evals against the same dataset.\n",
    "\n",
    "I'll call this eval `grade_appropriateness`, since that's what we're trying to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4646f4-235a-4261-af00-4830c845d30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eval(do_completion=True, name='grade_appropriateness', notes='', metrics=Metrics(function=[FunctionItem(name='is_role', depends_on=[], metric_level='Turn', kwargs={'role': 'assistant'}), FunctionItem(name='flesch_reading_ease', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})], rubric=[RubricItem(name='is_grade_appropriate', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})]), completion_llm=CompletionLlm(function_name='litellm_completion', include_system_prompt=True, kwargs={'model': 'gpt-4o-mini'}), grader_llm=GraderLlm(function_name='litellm_completion', kwargs={'model': 'gpt-4o-mini'}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = Eval(\n",
    "    name=\"grade_appropriateness\",\n",
    "    metrics=metrics,\n",
    "    grader_llm=grader_llm,\n",
    "    do_completion=True,\n",
    "    completion_llm=completion_llm,\n",
    ")\n",
    "eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2085f03-a760-4bc3-a883-ff40ed65f722",
   "metadata": {},
   "source": [
    "### Building an EvalRun\n",
    "\n",
    "To execute the defined {class}`~flexeval.schema.eval_schema.Eval`, we need to build an {class}`~flexeval.schema.evalrun_schema.EvalRun`.\n",
    "\n",
    "Here's what we need (other than the {class}`~flexeval.schema.eval_schema.Eval`):\n",
    " - {attr}`~flexeval.schema.evalrun_schema.EvalRun.data_sources` should be a list of datasets. We already saved a jsonl with our inputs, so we'll wrap the path in a {class}`~flexeval.schema.evalrun_schema.FileDataSource`.\n",
    " - {attr}`~flexeval.schema.evalrun_schema.EvalRun.database_path` defines the location of the SQLite file that FlexEval produces.\n",
    " - {attr}`~flexeval.schema.evalrun_schema.EvalRun.rubric_paths` is where we provide the rubric prompts we defined above (in the form of a {class}`~flexeval.schema.rubric_schema.RubricsCollection`).\n",
    " - {attr}`~flexeval.schema.evalrun_schema.EvalRun.config` is optional, but you can provide a {class}`~flexeval.schema.config_schema.Config` there to override settings that define how the Eval will be executed. In this case, we set `clear_tables` to True in order to delete any outputs in the provided `database_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b666ab22-5a44-4fd2-ba10-82bce189856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flexeval.schema import Config, EvalRun, FileDataSource, RubricsCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba16f1b0-60a7-4ca4-976f-6e55397958f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalRun(data_sources=[FileDataSource(name=None, notes=None, path=PosixPath('concept_queries.jsonl'), format='jsonl')], database_path=PosixPath('eval_results.db'), eval=Eval(do_completion=True, name='grade_appropriateness', notes='', metrics=Metrics(function=[FunctionItem(name='is_role', depends_on=[], metric_level='Turn', kwargs={'role': 'assistant'}), FunctionItem(name='flesch_reading_ease', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})], rubric=[RubricItem(name='is_grade_appropriate', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})]), completion_llm=CompletionLlm(function_name='litellm_completion', include_system_prompt=True, kwargs={'model': 'gpt-4o-mini'}), grader_llm=GraderLlm(function_name='litellm_completion', kwargs={'model': 'gpt-4o-mini'})), config=Config(logs_path=None, env_filepath=None, env={}, clear_tables=True, max_workers=1, random_seed_conversation_sampling=42, max_n_conversation_threads=50, nb_evaluations_per_thread=1, raise_on_completion_error=False, raise_on_metric_error=False), rubric_paths=[RubricsCollection(rubrics={'is_grade_appropriate': Rubric(prompt='Read the following input and output, assessing if the output is grade-appropriate.\\n[Input]: {context}\\n[Output]: {content}\\n\\nOn a new line after your explanation, print:\\n- YES if the Output is fully appropriate for the grade level\\n- NO if the Output would uses language or concepts that would be inappropriate for that grade level\\n\\nOnly print YES or NO on the final line.\\n', choice_scores={'YES': 1, 'NO': 0}, name=None, notes=None)})], function_modules=[<module 'flexeval.configuration.function_metrics' from '/Users/zacharylevonian/repos/FlexEval/src/flexeval/configuration/function_metrics.py'>], add_default_functions=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_sources = [FileDataSource(path=concept_queries_path)]\n",
    "database_path = Path(\"eval_results.db\")\n",
    "config = Config(clear_tables=True)\n",
    "eval_run = EvalRun(\n",
    "    data_sources=input_data_sources,\n",
    "    database_path=database_path,\n",
    "    eval=eval,\n",
    "    config=config,\n",
    "    rubric_paths=[RubricsCollection(rubrics=rubrics)],\n",
    ")\n",
    "eval_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e897396-c34a-47a1-b290-4db2cb22078c",
   "metadata": {},
   "source": [
    "### Running the EvalRun\n",
    "\n",
    "Once we've built an {class}`~flexeval.schema.evalrun_schema.EvalRun`, running it is easy: we can just use {func}`~flexeval.runner.run`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d51e3d-5b3c-470f-bb94-29e3dff4f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = flexeval.run(eval_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac15687-da31-49a3-9af0-e5a7514109a9",
   "metadata": {},
   "source": [
    "Now that we've run our Eval, we can analyze our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914afcd2-a9f4-49bb-9a2c-c0d8c489e353",
   "metadata": {},
   "source": [
    "## Part 2: Analyzing our results\n",
    "\n",
    "We'll analyze the data we created in Part 1.\n",
    "\n",
    "### flexeval.metrics.access\n",
    "\n",
    "A few utility functions are exposed in {mod}`flexeval.metrics.access` for returning computed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3710a57b-a4f4-42c7-89c3-46d784568667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython import display  # for prettier printing\n",
    "\n",
    "from flexeval import db_utils\n",
    "from flexeval.metrics import access as metric_access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afcae51-d28b-4948-8a46-60dc60cf8aae",
   "metadata": {},
   "source": [
    "Note: We use Peewee's database methods to access the output at {attr}`~flexeval.schema.evalrun_schema.EvalRun.database_path`. Using {func}`~flexeval.runner.run` will set the appropriate variables for you, but if you just want to access the data from an eval you can use {func}`flexeval.db_utils.ensure_database` to ensure the Peewee connection is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278c209b-b674-4290-b370-9b2be55472b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "database_path = Path(\"eval_results.db\")\n",
    "db_utils.ensure_database(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5fbf7-81e1-4024-8415-f0f01d3becca",
   "metadata": {},
   "source": [
    "To start, let's just get all of the metrics as a Pandas dataframe. \n",
    "\n",
    "{func}`~flexeval.metrics.access.get_all_metrics` returns a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b592c271-d138-4db5-9617-df98e2c2ac96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>evalsetrun</th>\n",
       "      <th>dataset</th>\n",
       "      <th>thread</th>\n",
       "      <th>turn</th>\n",
       "      <th>message</th>\n",
       "      <th>toolcall</th>\n",
       "      <th>evaluation_name</th>\n",
       "      <th>evaluation_type</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>source</th>\n",
       "      <th>depends_on</th>\n",
       "      <th>rubric_prompt</th>\n",
       "      <th>rubric_completion</th>\n",
       "      <th>rubric_model</th>\n",
       "      <th>rubric_completion_tokens</th>\n",
       "      <th>rubric_prompt_tokens</th>\n",
       "      <th>rubric_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>is_role</td>\n",
       "      <td>function</td>\n",
       "      <td>assistant</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'role': 'assistant'}</td>\n",
       "      <td>def is_role(object: Union[Turn, Message], role...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>is_role</td>\n",
       "      <td>function</td>\n",
       "      <td>assistant</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'role': 'assistant'}</td>\n",
       "      <td>def is_role(object: Union[Turn, Message], role...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>function</td>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>...</td>\n",
       "      <td>76.191959</td>\n",
       "      <td>{}</td>\n",
       "      <td>def flesch_reading_ease(turn: str) -&gt; float:\\n...</td>\n",
       "      <td>[{\"name\": \"is_role\", \"type\": null, \"kwargs\": {...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  evalsetrun  dataset  thread  turn message toolcall  \\\n",
       "0   1           1        1       1     1    None     None   \n",
       "1   2           1        1       1    13    None     None   \n",
       "2   3           1        1       1    13    None     None   \n",
       "\n",
       "       evaluation_name evaluation_type          metric_name  ... metric_value  \\\n",
       "0              is_role        function            assistant  ...     0.000000   \n",
       "1              is_role        function            assistant  ...     1.000000   \n",
       "2  flesch_reading_ease        function  flesch_reading_ease  ...    76.191959   \n",
       "\n",
       "                  kwargs                                             source  \\\n",
       "0  {'role': 'assistant'}  def is_role(object: Union[Turn, Message], role...   \n",
       "1  {'role': 'assistant'}  def is_role(object: Union[Turn, Message], role...   \n",
       "2                     {}  def flesch_reading_ease(turn: str) -> float:\\n...   \n",
       "\n",
       "                                          depends_on rubric_prompt  \\\n",
       "0                                                 []          None   \n",
       "1                                                 []          None   \n",
       "2  [{\"name\": \"is_role\", \"type\": null, \"kwargs\": {...          None   \n",
       "\n",
       "  rubric_completion rubric_model rubric_completion_tokens  \\\n",
       "0              None         None                      NaN   \n",
       "1              None         None                      NaN   \n",
       "2              None         None                      NaN   \n",
       "\n",
       "   rubric_prompt_tokens  rubric_score  \n",
       "0                   NaN          None  \n",
       "1                   NaN          None  \n",
       "2                   NaN          None  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(metric_access.get_all_metrics())\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2dce6-cb4c-4a91-906b-2ce49ea25065",
   "metadata": {},
   "source": [
    "We have access to all of the columns in {class}`~flexeval.classes.metric.Metric`.\n",
    "\n",
    "We can see that FlexEval generates one metric result each time it runs, along with information about what was run. Let's look at the columns we have for our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ae8075-4ff2-4f9f-abf4-1189d036965c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Columns returned by `get_all_metrics()`: id, evalsetrun, dataset, thread, turn, message, toolcall, evaluation_name, evaluation_type, metric_name, metric_level, metric_value, kwargs, source, depends_on, rubric_prompt, rubric_completion, rubric_model, rubric_completion_tokens, rubric_prompt_tokens, rubric_score"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Markdown(\"Columns returned by `get_all_metrics()`: \" + \", \".join(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc093470-96cd-4980-9d3f-c3f46f53a23f",
   "metadata": {},
   "source": [
    "Let's start by looking at how many times each metric was computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6791f442-9809-4642-8f33-5f860b068f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_role</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_grade_appropriate</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count\n",
       "evaluation_name            \n",
       "is_role                  24\n",
       "flesch_reading_ease      12\n",
       "is_grade_appropriate     12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.evaluation_name.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a1ffe-9920-44d2-a719-904418544657",
   "metadata": {},
   "source": [
    "These counts look right:\n",
    " - Our dataset contained 12 input sentences.\n",
    " - We generated 12 LLM responses.\n",
    " - `is_role` was evaluated on all 24 turns...\n",
    " - ...but `flesch_reading_ease` and `is_grade_appropriate` were computed only on the 12 LLM-generated (\"assistant\") turns.\n",
    "\n",
    "Let's start our analysis by looking at the numeric metric: Flesch reading ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83c648-c049-4a3a-aebd-1d9ca5419bda",
   "metadata": {},
   "source": [
    "### Analyzing a function metric\n",
    "\n",
    "A quick aside: Flesch reading ease is widely used, but it should be used with care. As summarized by Crossley et al. ([CLEAR 2022](https://link.springer.com/article/10.3758/s13428-022-01802-x)): \n",
    "\n",
    ">Traditional readability formulas lack construct and theoretical validity because they are based on weak proxies of word decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence) and ignore many text features that are important components of reading models including text cohesion and semantics. Additionally, many traditional readability formulas were normed using readers from specific age groups on small corpora of texts taken from specific domains.\n",
    "\n",
    "Nevertheless, many people may expect to see a readability score, and it can be useful to assess the [convergent validity](https://en.wikipedia.org/wiki/Convergent_validity) for some rubric metrics.\n",
    "\n",
    "Let's start by filtering to only the `flesch_reading_ease` metric results and computing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23e8aa5-b4bc-4217-ab9c-50c0b3c893ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.89\n"
     ]
    }
   ],
   "source": [
    "flesch_reading_ease = df[df.metric_name == \"flesch_reading_ease\"]\n",
    "print(f\"Mean Flesch reading ease score: {flesch_reading_ease.metric_value.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbd569-a801-4c92-b828-d0a0b3957f9c",
   "metadata": {},
   "source": [
    "A Flesch reading ease of 60 is at the [\"10th to 12th grade\" level](https://pages.stern.nyu.edu/wstarbuc/Writing/Flesch.htm).\n",
    "\n",
    "Hmm, seems a bit high. \n",
    "\n",
    "Plotting the distribution, we never seem to get above 80 (appropriate for 6th grade and lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63cb994-812f-40ff-838f-82aae44bc816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "\n",
    "ax.hist(flesch_reading_ease.metric_value, bins=np.arange(30, 110, 10))\n",
    "ax.set_xlabel(\"Flesch reading ease\")\n",
    "ax.set_ylabel(\"Concept explanation count\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5b2c5-d8b8-4f1a-a6a4-a0ce47b08536",
   "metadata": {},
   "source": [
    "Already, we've learned enough to consider going back to our prompt engineering to produce more age-appropriate messages.\n",
    "\n",
    "But presuming we're okay with the overall range, we'd prefer to see how things are looking at the original grade ranges we were targeting.\n",
    "\n",
    "We can extract the original grade from the initial user message in each thread in our dataset. \n",
    "To get those user messages, we can use another utility function in {mod}`flexeval.metrics.access`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df273c0e-2a5d-4c67-9a71-824a35483f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade_level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count\n",
       "grade_level       \n",
       "3                3\n",
       "5                3\n",
       "7                3\n",
       "9                3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread_ids = set(flesch_reading_ease.thread)\n",
    "# get the first message in each thread\n",
    "first_message_df = pd.DataFrame(\n",
    "    metric_access.get_first_user_message_for_threads(thread_ids)\n",
    ")\n",
    "# add the first_message_content column\n",
    "flesch_reading_ease = flesch_reading_ease.merge(\n",
    "    first_message_df[[\"thread\", \"content\"]].rename(\n",
    "        columns={\"content\": \"first_message_content\"}\n",
    "    ),\n",
    "    how=\"inner\",\n",
    "    on=\"thread\",\n",
    ")\n",
    "flesch_reading_ease[\"grade_level\"] = (\n",
    "    flesch_reading_ease.first_message_content.str.extract(\n",
    "        r\"(\\d+)(?:st|nd|rd|th)-grade\"\n",
    "    ).astype(int)\n",
    ")\n",
    "pd.DataFrame(flesch_reading_ease.grade_level.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2e28a-f3e8-4868-91b4-e8b440ca7dfb",
   "metadata": {},
   "source": [
    "With grade level in hand, we can investigate reading ease at each prompted grade level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3580c288-0682-48da-93a8-478204f3b34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2.5))\n",
    "\n",
    "# no has ever accused matplotlib of being concise\n",
    "ax.scatter(\n",
    "    flesch_reading_ease.grade_level,\n",
    "    flesch_reading_ease.metric_value,\n",
    "    color=\"black\",\n",
    "    s=6,\n",
    ")\n",
    "means = flesch_reading_ease.groupby(\"grade_level\").metric_value.mean()\n",
    "ax.scatter(\n",
    "    means.index, means, color=\"darkgreen\", s=50, marker=\"_\", label=\"Grade-level mean\"\n",
    ")\n",
    "for x, y in zip(means.index, means):\n",
    "    ax.text(x, y, f\"  {y:.1f}\", va=\"center\", ha=\"left\")\n",
    "ax.set_xticks(sorted(means.index))\n",
    "ax.set_xlim(means.index.min() - 1, means.index.max() + 1)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Grade\")\n",
    "ax.set_ylabel(\"Flesch reading ease\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25841739-aafb-491e-9e70-f30494e2b6f6",
   "metadata": {},
   "source": [
    "These group-level means suggest that the completions _are_ more grade-appropriate depending on the prompted grade.\n",
    "\n",
    "However, the reading ease scores also suggest the responses are potentially too complex.\n",
    "\n",
    "Even the most readable explanations â€“ generated for the third-grade prompt â€“ are at the 7th-grade level by the Flesch-Kincaid at the 7th-grade level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09f2e7-49c6-4c38-a336-cd879c552a96",
   "metadata": {},
   "source": [
    "### Analyzing a rubric metric\n",
    "\n",
    "We might attempt to go deeper by investigating the LLM-as-a-judge rubric results.\n",
    "\n",
    "One of the advantages of a rubric is that we can develop it iteratively.\n",
    "The investigation here is only an initial look; in reality, we would likely update the prompt after investigating the outputs and seeing where we might sharpen the validity of the metric we're developing.\n",
    "\n",
    "Let's start by looking at the raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c20e8a8-1da1-49e5-a0e8-af8b498a962e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_value</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "metric_value       \n",
       "1.0              11\n",
       "0.0               1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_grade_appropriate = df[df.metric_name == \"is_grade_appropriate\"]\n",
    "pd.DataFrame(is_grade_appropriate.metric_value.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fc0d3-d55b-4f4b-931a-3e681e430fcd",
   "metadata": {},
   "source": [
    "Only one determination of grade inappriateness! Let's try to get some insight by looking at one of the YES determinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5d89f35-e8b9-49a7-86f4-1ccf7ffc0357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompt:\n",
       "> Read the following input and output, assessing if the output is grade-appropriate.\n",
       "> [Input]: user: Concisely summarize factoring polynomials at the United States 9th-grade level.\n",
       "> \n",
       "> [Output]: assistant: Factoring polynomials involves breaking down a polynomial into simpler expressions, called factors, that can be multiplied to obtain the original polynomial. Here are the key steps:\n",
       "> \n",
       "> 1. **Look for a GCF (Greatest Common Factor):** Identify the largest factor common to all terms in the polynomial and factor it out.\n",
       "> \n",
       "> 2. **Identify Special Products:** Recognize patterns such as:\n",
       ">    - **Difference of squares:** \\(a^2 - b^2 = (a - b)(a + b)\\)\n",
       ">    - **Perfect square trinomial:** \\(a^2 + 2ab + b^2 = (a + b)^2\\) and \\(a^2 - 2ab + b^2 = (a - b)^2\\)\n",
       ">    - **Sum or difference of cubes:** \\(a^3 + b^3 = (a + b)(a^2 - ab + b^2)\\) and \\(a^3 - b^3 = (a - b)(a^2 + ab + b^2)\\)\n",
       "> \n",
       "> 3. **Factor Quadratics:** For polynomials in the form \\(ax^2 + bx + c\\), look for two numbers that multiply to \\(ac\\) and add to \\(b\\). Write it as \\((px + q)(rx + s)\\).\n",
       "> \n",
       "> 4. **Use the AC Method:** If factoring quadratics directly is tough, use the AC method, where you multiply \\(a\\) and \\(c\\), find two numbers that fit, and rewrite the middle term.\n",
       "> \n",
       "> 5. **Check Your Work:** After factoring, multiply the factors back together to see if you get the original polynomial.\n",
       "> \n",
       "> Factoring is useful for solving equations, simplifying expressions, and analyzing polynomial graphs.\n",
       "> \n",
       "> \n",
       "> On a new line after your explanation, print:\n",
       "> - YES if the Output is fully appropriate for the grade level\n",
       "> - NO if the Output would uses language or concepts that would be inappropriate for that grade level\n",
       "> \n",
       "> Only print YES or NO on the final line.\n",
       "\n",
       "Completion:\n",
       "> To assess if the output is grade-appropriate for 9th-grade students, we will evaluate the clarity, complexity, and relevance of the content provided.\n",
       "> \n",
       "> 1. **Target Audience Understanding**: 9th-grade students are typically around 14-15 years old and might have a basic understanding of algebra, including variables and polynomials.\n",
       "> \n",
       "> 2. **Content Review**:\n",
       ">    - The output defines what factoring polynomials is, which is suitable for the grade level.\n",
       ">    - It provides a clear list of steps which could help students follow the process.\n",
       ">    - It includes identifying the Greatest Common Factor (GCF), recognizing special products, factoring quadratics, and using methods like the AC method. These are all relevant and fundamental concepts in factoring at this level.\n",
       ">    - It uses terms like \"GCF\", \"difference of squares\", and includes examples which could aid understanding.\n",
       "> \n",
       "> 3. **Language and Complexity**: \n",
       ">    - The language used is straightforward and largely appropriate for 9th-grade students. However, some terms may require further explanation (like GCF or the AC method). But overall, the concepts should be accessible to students who have been exposed to these topics in class.\n",
       "> \n",
       "> 4. **General Guidance**: \n",
       ">    - The summary not only teaches how to factor but also emphasizes checking work, which is a good practice in mathematics. \n",
       "> \n",
       "> 5. **Conclusion**: \n",
       ">    - Considering all the points above, the output does not present any language or concepts that would be overly complex or inappropriate for a 9th-grade audience. \n",
       "> \n",
       "> Based on this detailed reasoning, I conclude that the output is appropriate for the grade level.\n",
       "> \n",
       "> YES \n",
       "> \n",
       "> YES\n",
       "\n",
       "Parsed response: 1.0 (YES)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quote_markdown(markdown_string: str) -> str:\n",
    "    return \"\\n\".join(\"> \" + line for line in markdown_string.splitlines())\n",
    "\n",
    "\n",
    "def format_rubric_summary(rubric_metric) -> str:\n",
    "    return f\"\"\"Prompt:\n",
    "{quote_markdown(rubric_metric.rubric_prompt)}\n",
    "\n",
    "Completion:\n",
    "{quote_markdown(rubric_metric.rubric_completion)}\n",
    "\n",
    "Parsed response: {rubric_metric.metric_value} ({rubric_metric.rubric_score})\"\"\"\n",
    "\n",
    "\n",
    "rubric_metric = (\n",
    "    is_grade_appropriate[is_grade_appropriate.metric_value == 1].sample(n=1).iloc[0]\n",
    ")\n",
    "display.Markdown(format_rubric_summary(rubric_metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7226ada-317f-49d2-8216-9c939c022a41",
   "metadata": {},
   "source": [
    "Hmm, not quite convincing. I already have a few ideas for how we might improve our prompt.\n",
    "\n",
    "Let's finish up by looking at the one No determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0768ddd3-04e8-4f05-b6e2-8e70db2f1adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Prompt:\n",
       "> Read the following input and output, assessing if the output is grade-appropriate.\n",
       "> [Input]: user: Concisely summarize factoring polynomials at the United States 3rd-grade level.\n",
       "> \n",
       "> [Output]: assistant: Factoring polynomials means breaking them down into simpler parts that, when multiplied together, give you the original polynomial. Think of it like taking a number apart into its smaller number pieces. For example, if you have \\( x^2 + 2x \\), you can factor it by pulling out a common piece, which in this case is \\( x \\). So, it becomes \\( x(x + 2) \\). This makes it easier to work with!\n",
       "> \n",
       "> \n",
       "> On a new line after your explanation, print:\n",
       "> - YES if the Output is fully appropriate for the grade level\n",
       "> - NO if the Output would uses language or concepts that would be inappropriate for that grade level\n",
       "> \n",
       "> Only print YES or NO on the final line.\n",
       "\n",
       "Completion:\n",
       "> 1. **Understanding the Input**: The user is asking for a summary of factoring polynomials suitable for a 3rd-grade level. This means the explanation needs to be simple, using language that is easily understandable by young children.\n",
       "> \n",
       "> 2. **Analyzing the Output**: The output explains that factoring polynomials involves breaking them down into simpler parts. It uses the example of \\( x^2 + 2x \\) and shows how to factor it into \\( x(x + 2) \\).\n",
       "> \n",
       "> 3. **Evaluating Language and Concepts**:\n",
       ">    - \"Factoring polynomials\" may be too advanced for a 3rd grader, who is typically learning basic operations with numbers and simple algebraic concepts.\n",
       ">    - The terminology could be challenging. Phrases like \"breaking them down into simpler parts\" and \"when multiplied together\" might be difficult for 3rd graders to grasp fully.\n",
       ">    - The example provided uses variables, which might also be unfamiliar to 3rd graders who may not yet have been introduced to algebraic expressions.\n",
       "> \n",
       "> 4. **Conclusion**: Overall, while the concept of breaking things down is appropriate, the specific language and the variable example make it likely outside the understanding of typical 3rd graders. \n",
       "> \n",
       "> Therefore, the output is not fully appropriate for the grade level of a 3rd grader.\n",
       "> \n",
       "> Based on this reasoning, I will provide the final answer.\n",
       "> \n",
       "> NO\n",
       "> \n",
       "> NO\n",
       "\n",
       "Parsed response: 0.0 (NO)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rubric_metric = (\n",
    "    is_grade_appropriate[is_grade_appropriate.metric_value == 0].sample(n=1).iloc[0]\n",
    ")\n",
    "display.Markdown(format_rubric_summary(rubric_metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee630e-8b77-43b1-a818-cf0e1b874e33",
   "metadata": {},
   "source": [
    "This analysis looks better, but still quite vague. I see many specifics that we could include in the prompt as examples!\n",
    "\n",
    "### Next steps\n",
    "\n",
    "Based on this analysis, I would:\n",
    " - Update the completion prompt to produce more appropriate responses.\n",
    " - Update the rubric prompt to make more accurate â€“ and harsher! â€“ determinations about the appropriateness of the langauge used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "mystnb": {
   "execution_mode": "off"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
