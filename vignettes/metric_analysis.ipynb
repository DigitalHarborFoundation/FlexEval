{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512a79f9-25ac-469e-9322-0b199e08436e",
   "metadata": {},
   "source": [
    "# Metric Analysis\n",
    "\n",
    "This vignette demonstrates accessing the results of a completed Eval Run.\n",
    "\n",
    "Author: Zachary Levonian \\\n",
    "Date: July 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177fe23-afd0-4548-8fb9-6073308a21cd",
   "metadata": {},
   "source": [
    "## Part 1: Running FlexEval to compute some metrics\n",
    "\n",
    "We'll create some test data, build an eval, and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efa0406-0987-42f7-b54a-6833244babbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "assert dotenv.load_dotenv(\"../.env\"), \"This vignette assumes access to API keys in a .env file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2425711-4fee-4d49-bb96-7592a24da32a",
   "metadata": {},
   "source": [
    "### Generating test data\n",
    "\n",
    "Let's evaluate the quality of grade-appropriate explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049336f0-7ede-489b-a45b-5dbb4374912a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concepts = [\"integer addition\", \"factoring polynomials\", \"logistic regression\"]\n",
    "grades = [\"3rd\", \"5th\", \"7th\", \"9th\"]\n",
    "\n",
    "user_queries = []\n",
    "for concept in concepts:\n",
    "    for grade in grades:\n",
    "        user_queries.append(\n",
    "            f\"Concisely summarize {concept} at the United States {grade}-grade level.\"\n",
    "        )\n",
    "len(user_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71700ecf-1a86-4363-80f8-4919c3c5bbd1",
   "metadata": {},
   "source": [
    "We can imagine that our system under test involves a particular system prompt, or perhaps multiple candidate prompts.\n",
    "\n",
    "In this case, we'll imagine a single, simple system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fdf148-885f-4fb4-bba4-f6f75e915d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a friendly math tutor.\n",
    "\n",
    "You attempt to summarize any mathematical topic the student is interested in, even if it's not appropriate for their grade level.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22992eee-f95e-4254-97b4-a5e7a326e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to JSONL\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "concept_queries_path = Path(\"concept_queries.jsonl\")\n",
    "with open(concept_queries_path, \"w\") as outfile:\n",
    "    for user_query in user_queries:\n",
    "        outfile.write(json.dumps({\"input\": [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_query}]}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68163ea3-f92c-485a-b5da-6f966c4e4e40",
   "metadata": {},
   "source": [
    "Each line of `concept_queries.jsonl` will become a unique {class}`~flexeval.classes.thread.Thread` to be processed.\n",
    "\n",
    "Now that we have test data, we can build a FlexEval configuration and execute it.\n",
    "\n",
    "### Defining an Eval\n",
    "\n",
    "An Eval describes the computations that need to happen to compute the required metrics.\n",
    "\n",
    "In this case, we'll set a few details:\n",
    " - We want to generate new LLM completions, rather than just using any existing assistant messages in our threads. To do that, we'll set {attr}`~flexeval.schema.eval_schema.Eval.do_completion` to true, and define the function to actually generate those completions from those provided in {mod}`flexeval.configuration.completion_functions`. In this case, we'll use {func}`~flexeval.configuration.completion_functions.litellm_completion`, which enables access to many different models.\n",
    " - We'll compute two {class}`~flexeval.schema.eval_schema.FunctionItem`s, a Flesch reading ease score and {meth}`~flexeval.configuration.function_metrics.is_role`. We need `is_role` because we can use its value to compute particular metrics only for assistant messages (like the new completions we'll be generating).\n",
    " - Finally, we can specify a custom {class}`~flexeval.schema.eval_schema.RubricItem`s. We'll write a prompt that describes the assessment we want to make. In this case, we try to determine if the assistant response is grade appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b72fa23-f077-434a-9916-e6389115560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flexeval\n",
    "from flexeval.schema import Eval, Rubric, GraderLlm, DependsOnItem, Metrics, FunctionItem, RubricItem, CompletionLlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a3afed-73e7-484d-9a6d-da6758e1716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by specifying an OpenAI model name here, we'll need OPENAI_API_KEY to exist in our environment variables or in our .env file\n",
    "completion_llm = CompletionLlm(\n",
    "    function_name=\"litellm_completion\",\n",
    "    kwargs={\"model\": \"gpt-4o-mini\", \"mock_response\": \"I can't help with that!\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ccc2343-0ed4-46fa-96ed-f13d09afd943",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rubric_prompt = \"\"\"Read the following input and output, assessing if the output is grade-appropriate.\n",
    "[Input]: {context}\n",
    "[Output]: {completion}\n",
    "\n",
    "On a new line after your explanation, print:\n",
    "- YES if the Output is fully appropriate for the grade level\n",
    "- SOMEWHAT if the Output uses some language or concepts that would be inappropriate for that grade level\n",
    "- NO if the Output would be mostly incomprehensible to a student at that grade level\n",
    "\n",
    "Only print YES, SOMEWHAT, or NO on the final line.\n",
    "\"\"\"\n",
    "rubric = Rubric(\n",
    "    prompt=bad_rubric_prompt,\n",
    "    choice_scores={\"YES\": 2, \"SOMEWHAT\": 1, \"NO\": 0},\n",
    ")\n",
    "grader_llm = GraderLlm(\n",
    "    function_name=\"litellm_completion\", \n",
    "    kwargs={\n",
    "        \"model\": \"gpt-4o-mini\"\n",
    "    }\n",
    ")\n",
    "rubrics = {\n",
    "    \"is_grade_appropriate\": rubric,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4646f4-235a-4261-af00-4830c845d30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eval(do_completion=True, name='grade_appropriateness', notes='', metrics=Metrics(function=[FunctionItem(name='is_role', depends_on=[], metric_level='Turn', kwargs={'role': 'assistant'}), FunctionItem(name='flesch_reading_ease', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})], rubric=[RubricItem(name='is_grade_appropriate', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})]), completion_llm=None, grader_llm=GraderLlm(function_name='openai_completion', kwargs={'model': 'gpt-4o-mini'}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_assistant_dependency = DependsOnItem(\n",
    "    name=\"is_role\", kwargs={\"role\": \"assistant\"}, metric_min_value=1\n",
    ")\n",
    "eval = Eval(\n",
    "    name=\"grade_appropriateness\",\n",
    "    metrics=Metrics(\n",
    "        function=[\n",
    "            FunctionItem(name=\"is_role\", kwargs={\"role\": \"assistant\"}),\n",
    "            FunctionItem(\n",
    "                name=\"flesch_reading_ease\",\n",
    "                depends_on=[is_assistant_dependency],\n",
    "            ),\n",
    "        ],\n",
    "        rubric=[RubricItem(name=\"is_grade_appropriate\", depends_on=[is_assistant_dependency])],\n",
    "    ),\n",
    "    grader_llm=grader_llm,\n",
    "    do_completion=True,\n",
    "    completion_llm=completion_llm,\n",
    ")\n",
    "eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2085f03-a760-4bc3-a883-ff40ed65f722",
   "metadata": {},
   "source": [
    "### Building an EvalRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b666ab22-5a44-4fd2-ba10-82bce189856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flexeval.schema import Config, EvalRun, FileDataSource, RubricsCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba16f1b0-60a7-4ca4-976f-6e55397958f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalRun(data_sources=[FileDataSource(name=None, notes=None, path=PosixPath('concept_queries.jsonl'), format='jsonl')], database_path=PosixPath('eval_results.db'), eval=Eval(do_completion=True, name='grade_appropriateness', notes='', metrics=Metrics(function=[FunctionItem(name='is_role', depends_on=[], metric_level='Turn', kwargs={'role': 'assistant'}), FunctionItem(name='flesch_reading_ease', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})], rubric=[RubricItem(name='is_grade_appropriate', depends_on=[DependsOnItem(name='is_role', type=None, kwargs={'role': 'assistant'}, metric_name=None, metric_level=None, relative_object_position=0, metric_min_value=1.0, metric_max_value=1.7976931348623157e+308)], metric_level='Turn', kwargs={})]), completion_llm=None, grader_llm=GraderLlm(function_name='openai_completion', kwargs={'model': 'gpt-4o-mini'})), config=Config(logs_path=None, env_filepath=None, env={}, clear_tables=True, max_workers=1, random_seed_conversation_sampling=42, max_n_conversation_threads=50, nb_evaluations_per_thread=1, raise_on_completion_error=False, raise_on_metric_error=False), rubric_paths=[RubricsCollection(rubrics={'is_grade_appropriate': Rubric(prompt='Read the following input and output, assessing if the output is grade-appropriate.\\n[Input]: {context}\\n[Output]: {completion}\\n\\nOn a new line after your explanation, print:\\n- YES if the Output is fully appropriate for the grade level\\n- SOMEWHAT if the Output uses some language or concepts that would be inappropriate for that grade level\\n- NO if the Output would be mostly incomprehensible to a student at that grade level\\n\\nOnly print YES, SOMEWHAT, or NO on the final line.\\n', choice_scores={'YES': 2, 'SOMEWHAT': 1, 'NO': 0}, name=None, notes=None)})], function_modules=[<module 'flexeval.configuration.function_metrics' from '/Users/zacharylevonian/repos/FlexEval/src/flexeval/configuration/function_metrics.py'>], add_default_functions=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_sources = [FileDataSource(path=concept_queries_path)]\n",
    "output_path = Path(\"eval_results.db\")\n",
    "config = Config(clear_tables=True)\n",
    "eval_run = EvalRun(\n",
    "    data_sources=input_data_sources,\n",
    "    database_path=output_path,\n",
    "    eval=eval,\n",
    "    config=config,\n",
    "    rubric_paths=[RubricsCollection(rubrics=rubrics)],\n",
    ")\n",
    "eval_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e897396-c34a-47a1-b290-4db2cb22078c",
   "metadata": {},
   "source": [
    "### Running the EvalRun\n",
    "\n",
    "Once we've built an EvalRun, running it is easy: we can just use {func}`~flexeval.runner.run`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d51e3d-5b3c-470f-bb94-29e3dff4f0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred generating completions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zacharylevonian/repos/FlexEval/src/flexeval/runner.py\", line 118, in run\n",
      "    completion = turn.get_completion(\n",
      "  File \"/Users/zacharylevonian/repos/FlexEval/src/flexeval/classes/turn.py\", line 37, in get_completion\n",
      "    if self.is_final_turn_in_input:\n",
      "AttributeError: 'Turn' object has no attribute 'is_final_turn_in_input'\n"
     ]
    }
   ],
   "source": [
    "_ = flexeval.run(eval_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaea401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "914afcd2-a9f4-49bb-9a2c-c0d8c489e353",
   "metadata": {},
   "source": [
    "## Part 2: Analyzing our results\n",
    "\n",
    "We'll analyze the data we created in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc96668-199c-4fed-9d69-ade24df05e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3710a57b-a4f4-42c7-89c3-46d784568667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flexeval.metrics import access as metric_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b592c271-d138-4db5-9617-df98e2c2ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 assistant 0.0\n",
      "2 2 assistant 0.0\n",
      "3 3 assistant 0.0\n",
      "4 4 assistant 0.0\n",
      "5 5 assistant 0.0\n",
      "6 6 assistant 0.0\n",
      "7 7 assistant 0.0\n",
      "8 8 assistant 0.0\n",
      "9 9 assistant 0.0\n",
      "10 10 assistant 0.0\n",
      "11 11 assistant 0.0\n",
      "12 12 assistant 0.0\n"
     ]
    }
   ],
   "source": [
    "for metric in metric_access.get_all_metrics():\n",
    "    print(\n",
    "        f\"{metric['thread']} {metric['turn']} {metric['metric_name']} {metric['metric_value']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae8075-4ff2-4f9f-abf4-1189d036965c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
