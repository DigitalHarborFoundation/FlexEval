#Instructions for running: 
#docker-compose run llm-evals python main.py EVAL_SUITE_NAME
#docker-compose up metabase

# example:
#   data:
#     path: data/test-cases/example.jsonl
#   function_metrics:
#     - name: string_length
#       score: all_by_role #aggregates across roles
#     - name: string_length
#       score: completion #string length of just completions

#   rubric_metrics:
#     - name: yeasayer-completion

#   grader_llm:
#     function_name: open_ai_completion # can be empty if not used, but don't delete the key
#     model_name: gpt-3.5-turbo
#     api_key_name: OPENAI_API_KEY #set in .env
#     n: 1

#   completion:
#     #this example uses a local instance of Jan (jan.ai) 
#     #hosting the mistral 7B model
#     function_name: open_ai_completion # can be empty if not used, but don't delete the key
#     model_name: gpt-3.5-turbo
#     api_key_name: OPENAI_API_KEY #set in .env
#     #function_name: jan_completion # defined in completion_functions.py
#     #endpoint: http://host.docker.internal:1337/v1/ #equivalent to localhost:1234/v1/
#     #model_name: mistral-ins-7b-q4
#     n: 2

cl:
  data:
    path: ../../data/test-cases/cl-test.jsonl
  function_metrics:
    - name: string_length
      score: per_turn_by_role
    - name: number_of_turns
      score: per_conversation_by_role
