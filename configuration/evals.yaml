#Instructions for running: 
#docker-compose run llm-evals python main.py EVAL_SUITE_NAME
#docker-compose up metabase


example-completion:
  data:
    - ../../data/test-cases/bot-adversarial-dataset-test-cases-first-5.jsonl

  do_completion: True

  name: my_eval
  notes: my notes
  #TODO - set parallelism here and overwrite the config.yaml if it's set

  metrics:
    function:
      - name: string_length
      - name: count_emojis
      # - name: count_unusual_function_calls
      #   depends:
      #     function_name: has_tool_call
      #     min_value: 1
      - name: flesch_kincaid_grade
      # - name: has_tool_call
      - name: openai_moderation_api
        
    rubric:
      - name: yeasayer-completion
        # depends:
        #   function_name: openai_moderation_api
        #   min_value: 0.1
        #   max_value: 1.0
      # - name: tool_call_is_in_response_to_request
      #   #this will only be run when ALL conditions are met
      #   depends:
      #     function_name: has_tool_call
      #     min_value: 1
      # - name: is_pedagocally_appropriate_plot
      #   #TWO conditions here must both be met
      #   depends:
      #     has_tool_call: 1
      #     tool_call_is_in_response_to_request: 1

      #TODO - will need to convert the metrics into a DAG and make sure they are evaluated in order
      #TODO - think about how to code values of dependencies so that matches are reliable

  
  completion_llm:
    function_name: open_ai_completion
    include_system_prompt: False
    kwargs:
      model_name: gpt-3.5-turbo
      api_key_name: OPENAI_API_KEY
      n: 2
    
  
  grader_llm:
    function_name: open_ai_completion
    model_name: gpt-4o
    api_key_name: OPENAI_API_KEY
  




cl:
  data:
    - ../../data/test-cases/cl-test.jsonl

  do_completion: False

  name: carnegie learning
  notes: my notes

  #anything in here will overwrite entries in src/llm-evals/config.yaml
  config:
    max_workers: 1

  metrics:
    function:
      - name: string_length
      - name: count_emojis
        depends_on:
          #list of dependencies
          - name: string_length
            min_value: 10
            max_value: 100

      # - name: count_unusual_function_calls
      #   depends:
      #     function_name: has_tool_call
      #     min_value: 1
      - name: flesch_kincaid_grade
      # - name: has_tool_call
      - name: openai_moderation_api
        
    # rubric:
    #   - 
    #   # - name: is_request_for_plot
      #   depends_on: 
      #     - name: count_emojis
      #       min_value: 1
    #       role: user
        # depends:
        #   function_name: openai_moderation_api
        #   min_value: 0.1
        #   max_value: 1.0
      # - name: tool_call_is_in_response_to_request
        #this will only be run when ALL conditions are met
        # depends:
        #   function_name: has_tool_call
        #   min_value: 1
      # - name: is_pedagocally_appropriate_plot
        #TWO conditions here must both be met
        # depends:
        #   has_tool_call: 1
        #   tool_call_is_in_response_to_request: 1

      #TODO - will need to convert the metrics into a DAG and make sure they are evaluated in order
      #TODO - think about how to code values of dependencies so that matches are reliable

  
  completion_llm:
    function_name: open_ai_completion
    include_system_prompt: False
    kwargs:
      model_name: gpt-3.5-turbo
      api_key_name: OPENAI_API_KEY
      n: 2
    
  
  grader_llm:
    function_name: open_ai_completion
    kwargs:
      model_name: gpt-4o
      api_key_name: OPENAI_API_KEY
  

